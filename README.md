Speech Emotion Recognition (SER) plays a pivotal role in augmenting human-computer interaction by enabling machines to detect and respond to human emotions based on speech. This study focuses on implementing SER using deep learning models, specifically Convolutional Neural Networks (CNN), Recurrent Neural Networks with Long Short-Term Memory (RNN-LSTM), and Multilayer Perceptron (MLP) classifiers. We utilize the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), a benchmark dataset comprising 7356 emotionally labeled audio files. Comprehensive feature extraction including Mel-frequency cepstral coefficients (MFCC), chroma, mel spectrogram, zero crossing rate (ZCR), and spectral centroid was performed using Librosa. Our experiments demonstrate that CNN achieves the highest classification accuracy of 73%, outperforming LSTM (71%) and MLP (62%). The results indicate the efficacy of deep learning techniques in capturing the temporal and spectral patterns inherent in speech for robust emotion classification.
